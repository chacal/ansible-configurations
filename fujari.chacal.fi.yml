---
#
# This playbook is used to provision Fujari after fresh Proxmox installation
# Assumptions:
#  - Passwordless SSH connection with "root"
#

- hosts: all
  remote_user: root
  gather_facts: yes

  vars_files:
    - secrets.yml

  vars:
    disks:
      - /dev/disk/by-id/scsi-36003005701ef594025e3130b929326b3  # /dev/sdc, 1.8TB HDD
      - /dev/disk/by-id/scsi-36003005701ef594025e3130b9293bb07  # /dev/sdc, 1.8TB HDD

  handlers:
    - name: Update boot parameters
      command: pve-efiboot-tool refresh
      shell: |

  roles:
    - role: sendgrid_with_dma
      sendgrid_api_key: "{{ chacal.sendgrid.fujari }}"
    - role: arc_size
      arc_cache_size: 3221225472
    - role: disable_log_compression
    - role: pve_common

  tasks:
    - name: Enable IOMMU
      copy:
        dest: /etc/kernel/cmdline
        content: root=ZFS=rpool/ROOT/pve-1 boot=zfs intel_iommu=on
      notify: Update boot parameters


    ######################################################
    #
    #  Setup ZFS hddpool and PVE storage pool in it
    #
    - name: Gather zpool facts
      zpool_facts:
      tags: storagepools

    - set_fact:
        hddpool_missing: "{{ 'hddpool' not in ansible_zfs_pools | map(attribute='name') }}"
      tags: storagepools

    - name: Confirm hddpool creation
      pause:
        prompt: |

          ##################################################################################
          ##
          ##  WARNING!! WARNING!! WARNING!! WARNING!!
          ##
          ##  ZFS pool "hddpool" doesn't exist!
          ##
          ##  Do you want to create it? This will WIPE ALL DATA on /dev/sdc and /dev/sdd!
          ##
          ##  Create hddpool? (yes/no) (Answering "no" skips the pool creation)
      register: create_hddpool
      when: hddpool_missing | bool
      tags: storagepools

    - name: Create ZFS pool "hddpool"
      block:
        - name: Remove existing partitions
          shell: |
            sgdisk --zap-all {{ item }}
          loop: "{{ vars.disks }}"
        - name: Remove potentially existing ZFS labels
          shell: |
            zpool labelclear -f {{ item }}
            sleep 2
          loop: "{{ vars.disks }}"
          ignore_errors: yes
        - name: Create hddpool pool
          shell: |
            zpool create -o ashift=12 \
                -O acltype=posixacl -O canmount=off -O compression=lz4 \
                -O dnodesize=auto -O normalization=formD -O relatime=on -O xattr=sa \
                hddpool \
                mirror {{ vars.disks | join(' ') }}
      when: hddpool_missing and create_hddpool.user_input | default(false) | bool
      tags: storagepools



    ######################################################
    #
    #  Setup ZFS snapshots using sanoid
    #
    - name: Install/update sanoid dependencies
      apt:
        state: latest
        cache_valid_time: 3600
        pkg:
          - libcapture-tiny-perl
          - libconfig-inifiles-perl
          - lzop
          - mbuffer
          - perl
          - pv
      tags: zfs-snapshots

    # Sanoid is installed from Sid .deb as it is not available for Buster
    - name: Install sanoid
      apt:
        deb: http://ftp.fi.debian.org/debian/pool/main/s/sanoid/sanoid_2.0.3-2_all.deb
      tags: zfs-snapshots

    # sanoid .deb also installs systemd timer -> use only that
    - name: Disable sanoid cron job
      file:
        path: /etc/cron.d/sanoid
        state: absent
      tags: zfs-snapshots

    - name: Create sanoid config dir
      file:
        path: /etc/sanoid
        state: directory
      tags: zfs-snapshots

    - name: Configure sanoid
      copy:
        dest: /etc/sanoid/sanoid.conf
        content: |
    - name: Setup ZFS datasets
      include_role: name=zfs_datasets
      vars:
        zfs_datasets:
          - { name: hddpool/nonbackupped, mountpoint: /hddpool/nonbackupped, canmount: on }
          - { name: rpool/nonbackupped, mountpoint: /rpool/nonbackupped, canmount: on }

    - name: Setup PVE storage pools
      include_role: name=pve_storagepools
      vars:
        pve_storagepools:
          - { name: "hdd-zfs-nonbackupped", dataset: hddpool/nonbackupped }
          - { name: "local-zfs-nonbackupped", dataset: rpool/nonbackupped }

          [rpool]
                  use_template = pve_system
                  recursive = yes

          [hddpool]
                  use_template = pve_system
                  recursive = yes

          [rpool/data]
                  use_template = vm_disks
                  recursive = yes

          [rpool/nonbackupped]
                  use_template = ignore

          [hddpool/nonbackupped]
                  use_template = ignore

          [template_pve_system]
                  frequent_period = 30
                  # 6h worth of 30min snapshots
                  frequently = 12
                  hourly = 48
                  daily = 21
                  monthly = 3
                  yearly = 0
                  autosnap = yes
                  autoprune = yes

          [template_vm_disks]
                  frequent_period = 30
                  # 24h worth of 30min snapshots
                  frequently = 48
                  hourly = 48
                  daily = 21
                  monthly = 12
                  yearly = 0
                  autosnap = yes
                  autoprune = yes

          [template_ignore]
                  autoprune = no
                  autosnap = no
                  monitor = no
      tags: zfs-snapshots
