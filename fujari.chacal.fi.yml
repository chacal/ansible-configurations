---
#
# This playbook is used to provision Fujari after fresh Proxmox installation
# Assumptions:
#  - Passwordless SSH connection with "root"
#

- hosts: all
  remote_user: root
  gather_facts: yes

  vars_files:
    - secrets.yml

  vars:
    disks:
      - /dev/disk/by-id/scsi-36003005701ef594025e3130b929326b3  # /dev/sdc, 1.8TB HDD
      - /dev/disk/by-id/scsi-36003005701ef594025e3130b9293bb07  # /dev/sdc, 1.8TB HDD

  handlers:
    - name: Update boot parameters
      command: pve-efiboot-tool refresh
    - name: Restart vmbr0
      shell: |
        ifdown vmbr0 || true
        ifup vmbr0 || true

  roles:
    - role: sendgrid_with_dma
      sendgrid_api_key: "{{ chacal.sendgrid.fujari }}"
    - role: arc_size
      arc_cache_size: 3221225472
    - role: disable_log_compression
    - role: pve_common

  tasks:
    - name: Enable IOMMU
      copy:
        dest: /etc/kernel/cmdline
        content: root=ZFS=rpool/ROOT/pve-1 boot=zfs intel_iommu=on
      notify: Update boot parameters


    ######################################################
    #
    #  Setup ZFS hddpool and PVE storage pool in it
    #
    - name: Gather zpool facts
      zpool_facts:
      tags: storagepools

    - set_fact:
        hddpool_missing: "{{ 'hddpool' not in ansible_zfs_pools | map(attribute='name') }}"
      tags: storagepools

    - name: Confirm hddpool creation
      pause:
        prompt: |

          ##################################################################################
          ##
          ##  WARNING!! WARNING!! WARNING!! WARNING!!
          ##
          ##  ZFS pool "hddpool" doesn't exist!
          ##
          ##  Do you want to create it? This will WIPE ALL DATA on /dev/sdc and /dev/sdd!
          ##
          ##  Create hddpool? (yes/no) (Answering "no" skips the pool creation)
      register: create_hddpool
      when: hddpool_missing | bool
      tags: storagepools

    - name: Create ZFS pool "hddpool"
      block:
        - name: Remove existing partitions
          shell: |
            sgdisk --zap-all {{ item }}
          loop: "{{ vars.disks }}"
        - name: Remove potentially existing ZFS labels
          shell: |
            zpool labelclear -f {{ item }}
            sleep 2
          loop: "{{ vars.disks }}"
          ignore_errors: yes
        - name: Create hddpool pool
          shell: |
            zpool create -o ashift=12 \
                -O acltype=posixacl -O canmount=off -O compression=lz4 \
                -O dnodesize=auto -O normalization=formD -O relatime=on -O xattr=sa \
                hddpool \
                mirror {{ vars.disks | join(' ') }}
      when: hddpool_missing and create_hddpool.user_input | default(false) | bool
      tags: storagepools

    - name: Setup ZFS datasets
      include_role: name=zfs_datasets
      vars:
        zfs_datasets:
          - { name: hddpool/nonbackupped, mountpoint: /hddpool/nonbackupped, canmount: on }
          - { name: rpool/nonbackupped, mountpoint: /rpool/nonbackupped, canmount: on }

    - name: Setup PVE storage pools
      include_role: name=pve_storagepools
      vars:
        pve_storagepools:
          - { name: "hdd-zfs-nonbackupped", dataset: hddpool/nonbackupped }
          - { name: "local-zfs-nonbackupped", dataset: rpool/nonbackupped }

    - name: Add PVE admin user
      include_role:
        name: pve_admin_user
      vars:
        pve_admin_username: pve_admin
        pve_admin_pw_hash: "{{ chacal.fujari.proxmox_user_pw_hash }}"
      tags: pve_admin_user

    - name: Setup Sanoid
      include_role: name=sanoid
      vars:
        sanoid_config: |
          [rpool]
                  use_template = pve_system
                  recursive = yes

          [hddpool]
                  use_template = pve_system
                  recursive = yes

          [rpool/data]
                  use_template = vm_disks
                  recursive = yes

          [rpool/nonbackupped]
                  use_template = ignore

          [hddpool/nonbackupped]
                  use_template = ignore

          [template_pve_system]
                  frequent_period = 30
                  # 6h worth of 30min snapshots
                  frequently = 12
                  hourly = 48
                  daily = 21
                  monthly = 3
                  yearly = 0
                  autosnap = yes
                  autoprune = yes

          [template_vm_disks]
                  frequent_period = 30
                  # 24h worth of 30min snapshots
                  frequently = 48
                  hourly = 48
                  daily = 21
                  monthly = 12
                  yearly = 0
                  autosnap = yes
                  autoprune = yes

          [template_ignore]
                  autoprune = no
                  autosnap = no
                  monitor = no

    - name: Install Zerotier for TRANSIT-ZT
      include_role:
        name: zerotier
      vars:
        zerotier_network_id: "{{ chacal.zerotier.transit.network_id }}"
        zerotier_moon_id: "{{ chacal.zerotier.transit.moon_id }}"

    - name: Setup networks
      copy:
        dest: /etc/network/interfaces
        content: |
          # network interface settings; autogenerated
          # Please do NOT modify this file directly, unless you know what
          # you're doing.
          #
          # If you want to manage parts of the network configuration manually,
          # please utilize the 'source' or 'source-directory' directives to do
          # so.
          # PVE will preserve these directives, but will NOT read its network
          # configuration from sourced files, so do not attempt to move any of
          # the PVE managed interfaces into external files!

          auto lo
          iface lo inet loopback

          auto eno2
          iface eno2 inet static
                  address 10.90.99.6/24
                  gateway 10.90.99.1
          #MGMT, Intel i217LM

          iface eno1 inet manual
          #Intel i210

          auto vmbr0
          iface vmbr0 inet manual
                  bridge-ports eno1 ztzlgndbao
                  bridge-stp off
                  bridge-fd 0
                  bridge-vlan-aware yes
                  bridge-vids 2-4094
          #SERVER

      notify: Restart vmbr0
