---
#
# This playbook is used to provision Fujari after fresh Proxmox installation
# Assumptions:
#  - Passwordless SSH connection with "root"
#
#
# NOTE NOTE NOTE NOTE:
# This is not totally up-to-date as hddpool now has two more disks! (2 x 1TB 2.5" HDD)
# These disks were added manually to the pool using command:
# zpool add hddpool -f mirror /dev/disk/by-id/ata-TOSHIBA_MQ01ABD100V_19OZPABGT /dev/disk/by-id/ata-ST1000LM024_HN-M101MBB_S2RXJ9FC501526
#

- hosts: all
  remote_user: root
  gather_facts: yes

  vars_files:
    - secrets.yml

  vars:
    disks:
      - /dev/disk/by-id/scsi-36003005701ef594025e3130b929326b3  # /dev/sdc, 1.8TB HDD
      - /dev/disk/by-id/scsi-36003005701ef594025e3130b9293bb07  # /dev/sdd, 1.8TB HDD

  handlers:
    - name: Update boot parameters
      command: pve-efiboot-tool refresh
    - name: Restart vmbr0
      shell: |
        ifdown vmbr0 || true
        ifup vmbr0 || true

  roles:
    - role: sendgrid_with_dma
      sendgrid_api_key: "{{ chacal.sendgrid.fujari }}"
    - role: arc_size
      arc_cache_size: 3221225472
    - role: disable_log_compression
    - role: pve_common
    - role: filebeat
      filebeat_elasticsearch_hosts:
        - "elastic.chacal.fi"
      filebeat_exclude_files:
        - "/var/log/pveproxy/*"
    - role: debian_common
      unattended_upgrades_origins: '"o=Proxmox";"o=elastic";'

  tasks:
    - name: Enable IOMMU
      copy:
        dest: /etc/kernel/cmdline
        content: root=ZFS=rpool/ROOT/pve-1 boot=zfs intel_iommu=on
      notify: Update boot parameters


    ######################################################
    #
    #  Setup ZFS hddpool and PVE storage pool in it
    #
    - name: Gather zpool facts
      zpool_facts:
      tags: storagepools

    - set_fact:
        hddpool_missing: "{{ 'hddpool' not in ansible_zfs_pools | map(attribute='name') }}"
      tags: storagepools

    - name: Confirm hddpool creation
      pause:
        prompt: |

          ##################################################################################
          ##
          ##  WARNING!! WARNING!! WARNING!! WARNING!!
          ##
          ##  ZFS pool "hddpool" doesn't exist!
          ##
          ##  Do you want to create it? This will WIPE ALL DATA on /dev/sdc and /dev/sdd!
          ##
          ##  Create hddpool? (yes/no) (Answering "no" skips the pool creation)
      register: create_hddpool
      when: hddpool_missing | bool
      tags: storagepools

    - name: Create ZFS pool "hddpool"
      block:
        - name: Remove existing partitions
          shell: |
            sgdisk --zap-all {{ item }}
          loop: "{{ vars.disks }}"
        - name: Remove potentially existing ZFS labels
          shell: |
            zpool labelclear -f {{ item }}
            sleep 2
          loop: "{{ vars.disks }}"
          ignore_errors: yes
        - name: Create hddpool pool
          shell: |
            zpool create -o ashift=12 \
                -O acltype=posixacl -O canmount=off -O compression=lz4 \
                -O dnodesize=auto -O normalization=formD -O relatime=on -O xattr=sa \
                hddpool \
                mirror {{ vars.disks | join(' ') }}
      when: hddpool_missing and create_hddpool.user_input | default(false) | bool
      tags: storagepools

    - name: Setup ZFS datasets
      include_role: name=zfs_datasets
      vars:
        zfs_datasets:
          - { name: hddpool/nonbackupped, mountpoint: /hddpool/nonbackupped, canmount: on }
          - { name: hddpool/backups, mountpoint: /hddpool/backups, canmount: on }
          - { name: hddpool/backups/wario, mountpoint: /hddpool/backups/wario, canmount: on }
          - { name: rpool/nonbackupped, mountpoint: /rpool/nonbackupped, canmount: on }

    - name: Setup PVE storage pools
      include_role: name=pve_storagepools
      vars:
        pve_storagepools:
          - { name: "hdd-zfs-nonbackupped", dataset: hddpool/nonbackupped }
          - { name: "local-zfs-nonbackupped", dataset: rpool/nonbackupped }

    - name: Add PVE admin user
      include_role:
        name: pve_admin_user
      vars:
        pve_admin_username: pve_admin
        pve_admin_pw_hash: "{{ chacal.fujari.proxmox_user_pw_hash }}"
      tags: pve_admin_user

    - name: Setup Sanoid
      include_role: name=sanoid
      vars:
        sanoid_config: |
          [rpool]
                  use_template = pve_system
                  recursive = yes

          [hddpool]
                  use_template = pve_system
                  recursive = yes

          [rpool/data]
                  use_template = vm_disks
                  recursive = yes

          [hddpool/backups]
                  use_template = backups
                  recursive = yes

          [rpool/nonbackupped]
                  use_template = ignore
                  recursive = yes

          [hddpool/nonbackupped]
                  use_template = ignore
                  recursive = yes

          [template_pve_system]
                  frequent_period = 30
                  # 6h worth of 30min snapshots
                  frequently = 12
                  hourly = 48
                  daily = 21
                  monthly = 3
                  yearly = 0
                  autosnap = yes
                  autoprune = yes

          [template_vm_disks]
                  frequent_period = 30
                  # 24h worth of 30min snapshots
                  frequently = 48
                  hourly = 48
                  daily = 21
                  monthly = 12
                  yearly = 0
                  autosnap = yes
                  autoprune = yes

          [template_backups]
                  monthly = 12
                  autoprune = yes
                  # Prune backups only if pool is > 70% full
                  prune_defer = 70
                  autosnap = no
                  monitor = no

          [template_ignore]
                  autoprune = no
                  autosnap = no
                  monitor = no
      tags: sanoid

    - name: Setup syncoid zfs dataset backups
      import_role:
        name: syncoid
      vars:
        syncoid_private_key: "{{ chacal.fujari.syncoid.private_key }}"
        syncoid_public_key: "{{ chacal.fujari.syncoid.public_key }}"
        syncoid_source: rpool
        syncoid_dst_host: wario.internal.chacal.fi
        syncoid_dst_dir: rpool/backups/fujari/rpool  # Parents of this dataset has to exist on fujari
        syncoid_excludes:
          - nonbackupped
      tags: syncoid

    - name: Install Zerotier for TRANSIT-ZT
      include_role:
        name: zerotier
      vars:
        zerotier_network_id: "{{ chacal.zerotier.transit.network_id }}"
        zerotier_moon_id: "{{ chacal.zerotier.transit.moon_id }}"
      tags: zerotier

    - name: Install Zerotier for MGMT
      include_role:
        name: zerotier
      vars:
        zerotier_network_id: "{{ chacal.zerotier.mgmt.network_id }}"
        zerotier_moon_id: "{{ chacal.zerotier.mgmt.moon_id }}"
        zerotier_allow_managed: false
      tags: zerotier

    - name: Install Zerotier for FREYA
      include_role:
        name: zerotier
      vars:
        zerotier_network_id: "{{ chacal.zerotier.freya.network_id }}"
        zerotier_moon_id: "{{ chacal.zerotier.freya.moon_id }}"
        zerotier_allow_managed: false
      tags: zerotier

    - name: Setup networks
      copy:
        dest: /etc/network/interfaces
        content: |
          # network interface settings; autogenerated
          # Please do NOT modify this file directly, unless you know what
          # you're doing.
          #
          # If you want to manage parts of the network configuration manually,
          # please utilize the 'source' or 'source-directory' directives to do
          # so.
          # PVE will preserve these directives, but will NOT read its network
          # configuration from sourced files, so do not attempt to move any of
          # the PVE managed interfaces into external files!

          auto lo
          iface lo inet loopback

          iface eno1 inet manual
          #vmbr0, Intel i210

          iface eno2 inet manual
          #mgmtbr0, Intel i217LM

          auto vmbr0
          iface vmbr0 inet manual
                  bridge-ports eno1 ztzlgndbao ztuku5opwi
                  bridge-stp off
                  bridge-fd 0
                  bridge-vlan-aware yes
                  bridge-vids 2-4094
                  # Remove all vlans from FREYA ZeroTier interface
                  post-up /usr/sbin/bridge vlan del vid 1-4094 dev ztuku5opwi
                  # Send VLAN 103 untagged to FREYA ZeroTier interface and tag incoming traffic as VLAN 103
                  post-up /usr/sbin/bridge vlan add vid 103 pvid 103 untagged dev ztuku5opwi
          #SERVER

          auto mgmtbr0
          iface mgmtbr0 inet static
                  address 10.90.99.6/24
                  gateway 10.90.99.1
                  bridge-ports eno2 ztuku43gze
                  bridge-stp off
                  bridge-fd 0
          #MGMT

      notify: Restart vmbr0

    - name: Set bridge interfaces up again to get the ZT interface connected to the bridges
      cron:
        name: "Setup {{ item }}"
        special_time: reboot
        job: "/usr/sbin/ifup {{ item }}"
      loop:
        - vmbr0
        - mgmtbr0

    - name: Set zfs_zevent_len_max
      import_role:
        name: zfs_zevent_len_max
